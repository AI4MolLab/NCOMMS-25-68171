{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841ff460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c54330",
   "metadata": {},
   "source": [
    "##### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c64e146",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_and_split_traces(npy_path, label, seed=1024):\n",
    "    data = np.load(npy_path, allow_pickle=True)  \n",
    "    traces = list(data)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(traces)\n",
    "    n_total = len(traces)\n",
    "    n_train = int(n_total * 0.7)\n",
    "    n_val = int(n_total * 0.15)\n",
    "    n_test = n_total - n_train - n_val\n",
    "    train_samples = [(t, label) for t in traces[:n_train]]\n",
    "    val_samples   = [(t, label) for t in traces[n_train:n_train + n_val]]\n",
    "    test_samples  = [(t, label) for t in traces[n_train + n_val:]]\n",
    "    return train_samples, val_samples, test_samples\n",
    "\n",
    "cup_paths = glob.glob(\"../data/CuP-0.1mM.npy\")\n",
    "cup_train_samples = []\n",
    "cup_val_samples = []\n",
    "cup_test_samples = []\n",
    "\n",
    "for path in cup_paths:\n",
    "    train_part, val_part, test_part = load_and_split_traces(path, label=0, seed=1024)\n",
    "    cup_train_samples.extend(train_part)\n",
    "    cup_val_samples.extend(val_part)\n",
    "    cup_test_samples.extend(test_part)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcef774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "caf_paths = glob.glob(\"../data/CuP+CAF-1_2(0.2mM).npy\")\n",
    "caf_train_samples = []\n",
    "caf_val_samples = []\n",
    "\n",
    "for path in caf_paths:\n",
    "    data = np.load(path, allow_pickle=True)\n",
    "    traces = list(data)  \n",
    "\n",
    "    random.seed(1024)\n",
    "    random.shuffle(traces)\n",
    "    n_total = len(traces)\n",
    "    n_train = int(n_total * 0.8)\n",
    "    n_val = n_total - n_train\n",
    "    caf_train_samples.extend([(t, 1) for t in traces[:n_train]])\n",
    "    caf_val_samples.extend([(t, 1) for t in traces[n_train:]])\n",
    "\n",
    "train_samples = cup_train_samples + caf_train_samples\n",
    "val_samples   = cup_val_samples   + caf_val_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f3d086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TraceDataset(Dataset):\n",
    "    def __init__(self, samples, max_len=1500):\n",
    "        self.samples = samples\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        trace, label = self.samples[idx]\n",
    "        trace = np.array(trace, dtype=np.float32)\n",
    "\n",
    "        if len(trace) > self.max_len:\n",
    "            trace = trace[:self.max_len]\n",
    "        else:\n",
    "            trace = np.pad(trace, (0, self.max_len - len(trace)), constant_values=0)\n",
    "        x_time = torch.tensor(trace).unsqueeze(0)  \n",
    "        freq = np.fft.rfft(trace)\n",
    "        mag = np.abs(freq)\n",
    "        x_freq = torch.tensor(mag[np.newaxis, :], dtype=torch.float32)\n",
    "\n",
    "        return x_time, x_freq, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b10719",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c028ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class TFC(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super(TFC, self).__init__()\n",
    "        self.encoder_time = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=9, padding=4),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm([64, 1500]),\n",
    "            nn.MaxPool1d(2), \n",
    "\n",
    "            nn.Conv1d(64, 128, kernel_size=5, padding=2),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm([128, 750]),\n",
    "\n",
    "            nn.Conv1d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm([128, 750]),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 750, 128),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "\n",
    "        self.projector_t = nn.Sequential(\n",
    "            nn.Conv1d(128, 64, kernel_size=1),\n",
    "            nn.LayerNorm([64, 1]),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.encoder_freq = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=7, padding=3),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm([64, 751]),\n",
    "            nn.MaxPool1d(2), \n",
    "\n",
    "            nn.Conv1d(64, 128, kernel_size=5, padding=2),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm([128, 375]),\n",
    "\n",
    "            nn.Conv1d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm([128, 375]),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 375, 128),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "\n",
    "        self.projector_f = nn.Sequential(\n",
    "            nn.Conv1d(128, 64, kernel_size=1),\n",
    "            nn.LayerNorm([64, 1]),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "    def forward(self, x_in_t, x_in_f):\n",
    "        h_t = self.encoder_time(x_in_t)      \n",
    "        h_t = h_t.unsqueeze(-1)              \n",
    "        z_t = self.projector_t(h_t)           \n",
    "\n",
    "        h_f = self.encoder_freq(x_in_f)       \n",
    "        h_f = h_f.unsqueeze(-1)               \n",
    "        z_f = self.projector_f(h_f)           \n",
    "\n",
    "        return z_t, z_f\n",
    "\n",
    "class target_classifier(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super(target_classifier, self).__init__()\n",
    "        self.logits = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, z_t, z_f):\n",
    "        z = torch.cat([z_t, z_f], dim=1)  \n",
    "        out = self.logits(z)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370caad3",
   "metadata": {},
   "source": [
    "##### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef99b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,classification_report,accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from collections import Counter\n",
    "import matplotlib.colors as mcolors\n",
    "def ConsistencyLoss(z1, z2):\n",
    "    z1 = F.normalize(z1, dim=1)\n",
    "    z2 = F.normalize(z2, dim=1)\n",
    "    return F.mse_loss(z1, z2)\n",
    "def train_one_epoch(tfc_model, classifier, dataloader, optimizer, criterion, contrastive_weight=0.1):\n",
    "    tfc_model.train()\n",
    "    classifier.train()\n",
    "    total_loss = 0\n",
    "    for x_t, x_f, y in dataloader:\n",
    "        x_t, x_f, y = x_t.to(device), x_f.to(device), y.to(device)\n",
    "        z_t, z_f = tfc_model(x_t, x_f)\n",
    "        logits = classifier(z_t, z_f)\n",
    "        ce_loss = criterion(logits, y)\n",
    "        loss_c = ConsistencyLoss(F.normalize(z_t, dim=1), F.normalize(z_f, dim=1))\n",
    "        loss = ce_loss + 0.2 * loss_c\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(tfc_model, classifier, dataloader, return_acc=False):\n",
    "\n",
    "    tfc_model.eval()\n",
    "    classifier.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_t, x_f, y in dataloader:\n",
    "            x_t, x_f = x_t.to(device), x_f.to(device)\n",
    "            z_t, z_f = tfc_model(x_t, x_f)\n",
    "            logits = classifier(z_t, z_f)\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(y.numpy())\n",
    "\n",
    "    print(\"\\n Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, digits=4))\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"\\n Accuracy: {acc:.4f}\")\n",
    "    \n",
    "    cm_normalized = confusion_matrix(all_labels, all_preds, normalize='true')\n",
    "    \n",
    "    display_labels = [\"CuP\", \"CuP+CAF\"]\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    colors = [\"#FFFFFF\", \"#F3A697\", \"#F17D65\"]\n",
    "    cmap_custom = mcolors.LinearSegmentedColormap.from_list(\"white_to_red\", colors)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_normalized,\n",
    "                                  display_labels=display_labels)\n",
    "\n",
    "    disp.plot(cmap=cmap_custom, values_format='.3f', ax=ax, colorbar=False)\n",
    "    im = disp.im_\n",
    "    im.set_clim(0, 1)\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 20,\n",
    "        # 'font.family': 'Arial',\n",
    "        'xtick.labelsize': 20,\n",
    "        'ytick.labelsize': 20\n",
    "    })\n",
    "\n",
    "    cbar = fig.colorbar(im, ax=ax)\n",
    "    cbar.ax.tick_params(labelsize=20)     \n",
    "    cbar.ax.yaxis.label.set_size(22)      \n",
    "    \n",
    "    plt.setp(ax.get_yticklabels(), ha=\"right\", multialignment=\"center\", fontsize=20)\n",
    "    ax.set_xlabel(\"Predicted label\", fontsize=22, fontweight='normal')\n",
    "    ax.set_ylabel(\"True label\", fontsize=22, fontweight='normal')\n",
    "    plt.setp(ax.get_xticklabels(), ha='center', fontsize=20)\n",
    "    plt.setp(ax.get_yticklabels(), rotation=90, va='center', fontsize=20)\n",
    "\n",
    "    \n",
    "    for text in disp.ax_.texts:\n",
    "        text.set_fontsize(22)\n",
    "        bg_val = disp.confusion_matrix[int(text.get_position()[1])][int(text.get_position()[0])]\n",
    "        if bg_val > 0.5:\n",
    "            text.set_color('white')\n",
    "        else:\n",
    "            text.set_color('black')\n",
    "\n",
    "    plt.tight_layout()\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    if return_acc:\n",
    "        return acc\n",
    "\n",
    "\n",
    "def predict_class_distribution(tfc_model, classifier, dataloader, device, class_names=[\"CuP\", \"CAF\"]):\n",
    "    tfc_model.eval()\n",
    "    classifier.eval()\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for x_t, x_f, _ in dataloader:\n",
    "            x_t, x_f = x_t.to(device), x_f.to(device)\n",
    "            z_t, z_f= tfc_model(x_t, x_f)  \n",
    "            logits = classifier(z_t, z_f)            \n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "\n",
    "    counter = Counter(all_preds)\n",
    "    total = sum(counter.values())\n",
    "    for cls_idx in sorted(counter.keys()):\n",
    "        count = counter[cls_idx]\n",
    "        percent = 100 * count / total\n",
    "        cls_name = class_names[cls_idx] if cls_idx < len(class_names) else f\"Class {cls_idx}\"\n",
    "        print(f\"{cls_name:>6} ：Proportion {percent:.2f}%\")\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273434e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "train_loader = DataLoader(TraceDataset(train_samples), batch_size=32, shuffle=True, drop_last=True)\n",
    "val_loader   = DataLoader(TraceDataset(val_samples), batch_size=32, shuffle=False, drop_last=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class Config:\n",
    "    TSlength_aligned_t = 1500\n",
    "    TSlength_aligned_f = 751\n",
    "    num_classes_target = 2\n",
    "configs = Config()\n",
    "\n",
    "tfc_model = TFC(configs).to(device)\n",
    "classifier = target_classifier(configs).to(device)\n",
    "optimizer = torch.optim.Adam(list(tfc_model.parameters()) + list(classifier.parameters()), lr=4e-4)\n",
    "num_cup = sum(label == 0 for _, label in train_samples)\n",
    "num_caf = sum(label == 1 for _, label in train_samples)\n",
    "total = num_cup + num_caf\n",
    "print(\"Train set: CuP=\", num_cup, \", CAF=\", num_caf)\n",
    "\n",
    "weights = torch.tensor([\n",
    "    total / num_cup,\n",
    "    total / num_caf\n",
    "], dtype=torch.float32).to(device)\n",
    "print(f\"Using class weights: CuP={weights[0]:.2f}, CAF={weights[1]:.2f}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "for epoch in range(1, 21):\n",
    "    loss = train_one_epoch(tfc_model, classifier, train_loader, optimizer, criterion)\n",
    "    print(f\"[Epoch {epoch}] Loss: {loss:.4f}\")\n",
    "    evaluate(tfc_model, classifier, val_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586654a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trace_level_samples(file_label_pairs, max_len):\n",
    "    samples = []\n",
    "    for path, label in file_label_pairs:\n",
    "        data = np.load(path, allow_pickle=True) \n",
    "        for trace in data:\n",
    "            trace = np.array(trace, dtype=np.float32)\n",
    "            if len(trace) > max_len:\n",
    "                trace = trace[:max_len]\n",
    "            else:\n",
    "                pad_len = max_len - len(trace)\n",
    "                trace = np.concatenate([trace, np.zeros(pad_len, dtype=np.float32)])\n",
    "            samples.append((trace, label))\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c870ba80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "final_test_samples = [(trace, 0) for trace, _ in cup_test_samples]\n",
    "final_test_loader = DataLoader(TraceDataset(final_test_samples), batch_size=32, shuffle=False)\n",
    "evaluate(tfc_model, classifier, final_test_loader)\n",
    "predict_class_distribution(tfc_model, classifier, final_test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4dcf02",
   "metadata": {},
   "source": [
    "##### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0618133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "manual_paths = [\n",
    "    (\"../data/CuP+CAF-1_1(0.1mM).npy\", 1E-4),\n",
    "    (\"../data/CuP+CAF-1_0.5(0.05mM).npy\", 5E-5),\n",
    "    (\"../data/CuP+CAF-10_1(0.01mM).npy\", 1E-5),\n",
    "    (\"../data/CuP+CAF-100_1(0.001mM).npy\", 1E-6),\n",
    "    (\"../data/CuP+CAF-1000_1(0.0001mM).npy\", 1E-7),\n",
    "    (\"../data/CuP+CAF-10000_1(0.00001mM).npy\", 1E-8),\n",
    "    (\"../data/CuP+CAF-100000_1(0.000001mM).npy\", 1E-9),\n",
    "    (\"../data/CuP+CAF-1000000_1(0.0000001mM).npy\", 1E-10),\n",
    "    (\"../data/CuP+CAF-10000000_1(0.00000001mM).npy\", 1E-11),\n",
    "    (\"../data/CuP+CAF-10^10_1(10^-14mM).npy\", 1E-14),\n",
    "    (\"../data/CuP+CAF-10^12_1(10^-16mM).npy\", 1E-16),\n",
    "    (\"../data/CuP+CAF-10^14_1(10^-18mM).npy\", 1E-18),\n",
    "\n",
    "]\n",
    "\n",
    "def load_trace_level_samples(file_label_pairs, max_len=1500):\n",
    "    samples = []\n",
    "    for path, label in file_label_pairs:\n",
    "        data = np.load(path, allow_pickle=True)\n",
    "        for trace in data:\n",
    "            trace = np.array(trace, dtype=np.float32)\n",
    "            if len(trace) > max_len:\n",
    "                trace = trace[:max_len]\n",
    "            else:\n",
    "                trace = np.pad(trace, (0, max_len - len(trace)), constant_values=0)\n",
    "            samples.append((trace, label))\n",
    "    return samples\n",
    "concentration_list = []\n",
    "caf_proportion_list = []\n",
    "\n",
    "for path, conc in manual_paths:\n",
    "    try:\n",
    "        samples = load_trace_level_samples([(path, 1)], max_len=1500)\n",
    "        if len(samples) == 0:\n",
    "            continue\n",
    "        loader = DataLoader(TraceDataset(samples), batch_size=32, shuffle=False, drop_last=True)\n",
    "\n",
    "        tfc_model.eval()\n",
    "        classifier.eval()\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for x_t, x_f, _ in loader:\n",
    "                x_t, x_f = x_t.to(device), x_f.to(device)\n",
    "                z_t, z_f = tfc_model(x_t, x_f)\n",
    "                out = classifier(z_t, z_f)\n",
    "                pred = torch.argmax(out, dim=1).cpu().numpy()\n",
    "                preds.extend(pred.tolist())\n",
    "        count = Counter(preds)\n",
    "        total = len(preds)\n",
    "        ratio = count.get(1, 0) / total\n",
    "        print(f\"{conc:.1e} mM:  CAF Ratio = {ratio:.2f}\")\n",
    "\n",
    "        concentration_list.append(conc)\n",
    "        caf_proportion_list.append(ratio)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0562b9aa",
   "metadata": {},
   "source": [
    "##### Limit of Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ec3b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "from scipy.stats import linregress\n",
    "from matplotlib.ticker import FuncFormatter, LogLocator, NullFormatter\n",
    "\n",
    "valid_indices = [i for i, c in enumerate(concentration_list) if c > 0]\n",
    "concentration_array = np.array(concentration_list)[valid_indices]\n",
    "proportion_array = np.array(caf_proportion_list)[valid_indices]\n",
    "\n",
    "log_conc = np.log10(concentration_array)\n",
    "slope, intercept, r_value, _, _ = linregress(log_conc, proportion_array)\n",
    "log_x_min = np.log10(min(concentration_array))\n",
    "log_x_max = np.log10(max(concentration_array))\n",
    "fit_x = np.linspace(log_x_min, log_x_max, 100)\n",
    "fit_y = slope * fit_x + intercept\n",
    "\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.size': 20,\n",
    "    # 'font.family': 'Arial',\n",
    "    'xtick.labelsize': 20,\n",
    "    'ytick.labelsize': 20\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(concentration_array, proportion_array, \n",
    "           marker='o', s=180, c='#F17D65', alpha=0.8, edgecolors='none',zorder=10, label='CuP+CAF')\n",
    "ax.plot(10**fit_x, fit_y, '-', color='black', lw=2.5)\n",
    "\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.set_xlim(min(concentration_array) * 0.1, max(concentration_array) * 10)\n",
    "ax.set_xlabel(\"CAF concentration / mol/L\", fontsize=24, fontweight='normal')\n",
    "ax.set_ylabel(\"CAF ratio\", fontsize=24, fontweight='normal')\n",
    "\n",
    "def sci_notation_formatter(x, pos):\n",
    "    if x == 0:\n",
    "        return \"0\"\n",
    "    exponent = int(np.log10(x))\n",
    "    return r\"$10^{{{}}}$\".format(exponent)\n",
    "ax.xaxis.set_major_locator(LogLocator(base=10.0, subs=[1.0]))\n",
    "ax.xaxis.set_major_formatter(FuncFormatter(sci_notation_formatter))\n",
    "plt.setp(ax.get_xticklabels(), rotation=0, ha=\"center\")\n",
    "\n",
    "for spine in ['top', 'bottom', 'left', 'right']:\n",
    "    ax.spines[spine].set_visible(True)\n",
    "    ax.spines[spine].set_linewidth(1.5)\n",
    "ax.tick_params(width=1.5, length=6)\n",
    "\n",
    "sign = '+' if intercept >= 0 else '-'\n",
    "equation_text = (\n",
    "    f\"y = {slope:.2f}x {sign} {abs(intercept):.2f}\\n\"\n",
    "    f\"$R^2$ = {r_value**2:.2f}\"\n",
    ")\n",
    "ax.text(0.05, 0.65, equation_text,\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=24,\n",
    "        verticalalignment='top')\n",
    "\n",
    "ax.legend(loc='upper left', fontsize=24, frameon=False, handletextpad=0.1, markerscale=1.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a26d91",
   "metadata": {},
   "source": [
    "##### Response Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b1d5bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "from scipy.stats import linregress\n",
    "from collections import Counter\n",
    "\n",
    "sample_sizes_to_test = [40,50,66, 79,200, 'all']\n",
    "set_seed(43)\n",
    "output_dir = \"analysis_by_sample_size\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for n_samples in sample_sizes_to_test:\n",
    "    \n",
    "    concentration_list = []\n",
    "    caf_proportion_list = []\n",
    "    for path, conc in manual_paths:\n",
    "\n",
    "        all_samples = load_trace_level_samples([(path, 1)], max_len=1500)\n",
    "        if n_samples == 'all':\n",
    "            samples_to_use = all_samples\n",
    "        else:\n",
    "            num_to_take = min(n_samples, len(all_samples))\n",
    "            samples_to_use = random.sample(all_samples, num_to_take)\n",
    "\n",
    "        if not samples_to_use:\n",
    "            continue\n",
    "\n",
    "        loader = DataLoader(TraceDataset(samples_to_use), batch_size=32, shuffle=False, drop_last=True)\n",
    "        \n",
    "        tfc_model.eval()\n",
    "        classifier.eval()\n",
    "        preds = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x_t, x_f, _ in loader:\n",
    "                x_t, x_f = x_t.to(device), x_f.to(device)\n",
    "                z_t, z_f = tfc_model(x_t, x_f)\n",
    "                out = classifier(z_t, z_f)\n",
    "                pred = torch.argmax(out, dim=1).cpu().numpy()\n",
    "                preds.extend(pred.tolist())\n",
    "        \n",
    "\n",
    "        count = Counter(preds)\n",
    "        total = len(preds)\n",
    "        ratio = count.get(1, 0) / total \n",
    "        print(f\"{conc:.1e} mM: CAF Ratio = {ratio:.3f}\")\n",
    "\n",
    "        concentration_list.append(conc)\n",
    "        caf_proportion_list.append(ratio)\n",
    "\n",
    "    valid_indices = [i for i, c in enumerate(concentration_list) if c > 0]\n",
    "    concentration_array = np.array(concentration_list)[valid_indices]\n",
    "    proportion_array = np.array(caf_proportion_list)[valid_indices]\n",
    "    \n",
    "\n",
    "    log_conc = np.log10(concentration_array)\n",
    "    slope, intercept, r_value, _, _ = linregress(log_conc, proportion_array)\n",
    "    log_x_min = np.log10(min(concentration_array))\n",
    "    log_x_max = np.log10(max(concentration_array))\n",
    "    fit_x = np.linspace(log_x_min, log_x_max, 100)\n",
    "    fit_y = slope * fit_x + intercept\n",
    "\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 20,\n",
    "        # 'font.family': 'Arial',\n",
    "        'xtick.labelsize': 20,\n",
    "        'ytick.labelsize': 20\n",
    "    })\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    label = f'CuP+CAF (n = {n_samples})' if n_samples != 'all' else 'CuP+CAF (entire dataset)'\n",
    "\n",
    "    ax.scatter(concentration_array, proportion_array, \n",
    "               marker='o', s=180, c='#F17D65', alpha=0.8, edgecolors='none',zorder=10, label=label)\n",
    "    ax.plot(10**fit_x, fit_y, '-', color='black', lw=2.5)\n",
    "\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.set_xlim(min(concentration_array) * 0.1, max(concentration_array) * 10)\n",
    "    ax.set_xlabel(\"CAF concentration / mol/L\", fontsize=24)\n",
    "    ax.set_ylabel(\"CAF ratio\", fontsize=24) \n",
    "\n",
    "    def sci_notation_formatter(x, pos):\n",
    "        if x == 0:\n",
    "            return \"0\"\n",
    "        exponent = int(np.log10(x))\n",
    "        return r\"$10^{{{}}}$\".format(exponent)\n",
    "\n",
    "    ax.xaxis.set_major_locator(LogLocator(base=10.0, subs=[1.0]))\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(sci_notation_formatter))\n",
    "    plt.setp(ax.get_xticklabels(), rotation=0, ha=\"center\")\n",
    "\n",
    "    for spine in ['top', 'bottom', 'left', 'right']:\n",
    "        ax.spines[spine].set_visible(True)\n",
    "        ax.spines[spine].set_linewidth(1.5)\n",
    "    ax.tick_params(width=1.5, length=6)\n",
    "\n",
    "    sign = '+' if intercept >= 0 else '-'\n",
    "    equation_text = (\n",
    "        f\"y = {slope:.2f}x {sign} {abs(intercept):.2f}\\n\"\n",
    "        f\"$R^2$ = {r_value**2:.2f}\"\n",
    "    )\n",
    "    ax.text(0.05, 0.65, equation_text,\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=24,\n",
    "            verticalalignment='top')\n",
    "\n",
    "    ax.legend(loc='upper left',fontsize=24, frameon=False, handletextpad=0.1, markerscale=1.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15e62ef",
   "metadata": {},
   "source": [
    "##### Response Time-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bdf2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "set_seed(42)\n",
    "MAX_N              = 1600                          \n",
    "SAMPLING_FREQ      = 20000              \n",
    "TRACE_LENGTH       = 1500               \n",
    "DECISION_TIME_S    = 5.0                \n",
    "START_N_MIN        = 10                 \n",
    "all_data_by_conc = {}\n",
    "min_samples_across_all_files = float('inf')\n",
    "\n",
    "for path, conc in manual_paths:\n",
    "    samples = load_trace_level_samples([(path, 1)], max_len=TRACE_LENGTH)\n",
    "    all_data_by_conc[conc] = samples\n",
    "    min_samples_across_all_files = min(min_samples_across_all_files, len(samples))\n",
    "\n",
    "MAX_N = min(MAX_N, min_samples_across_all_files)\n",
    "time_per_trace = TRACE_LENGTH / SAMPLING_FREQ   \n",
    "\n",
    "def build_sample_points(max_n,\n",
    "                        start_n=START_N_MIN,\n",
    "                        time_per_trace=time_per_trace,\n",
    "                        decision_time_s=DECISION_TIME_S,\n",
    "                        num_points=38,      \n",
    "                        power=2.4,          \n",
    "                        min_step=4):        \n",
    "    t_min = start_n * time_per_trace\n",
    "    t_max = max_n  * time_per_trace\n",
    "    u = np.linspace(0.0, 1.0, num_points)\n",
    "    t = t_min + (t_max - t_min) * (u ** power)\n",
    "    n = np.rint(t / time_per_trace).astype(int)\n",
    "    decision_n = int(round(decision_time_s / time_per_trace))\n",
    "    n = np.concatenate(([start_n], n, [max_n, decision_n]))\n",
    "    n = np.unique(n)\n",
    "    n = n[(n >= start_n) & (n <= max_n)]\n",
    "    n_sorted = [int(n[0])]\n",
    "    for k in n[1:]:\n",
    "        if k - n_sorted[-1] >= min_step:\n",
    "            n_sorted.append(int(k))\n",
    "    return n_sorted\n",
    "\n",
    "sample_points = build_sample_points(MAX_N)\n",
    "time_points   = [n * time_per_trace for n in sample_points]\n",
    "\n",
    "def _predict_ratio_for_conc(n, all_samples):\n",
    "    k = min(n, len(all_samples))\n",
    "    samples_to_use = random.sample(all_samples, k)\n",
    "    loader = DataLoader(TraceDataset(samples_to_use), batch_size=32, shuffle=False, drop_last=False)\n",
    "    if len(loader) == 0:\n",
    "        return None\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for x_t, x_f, _ in loader:\n",
    "            x_t, x_f = x_t.to(device), x_f.to(device)\n",
    "            z_t, z_f = tfc_model(x_t, x_f)\n",
    "            out = classifier(z_t, z_f)\n",
    "            pred = torch.argmax(out, dim=1).cpu().numpy()\n",
    "            preds.extend(pred.tolist())\n",
    "    if not preds:\n",
    "        return None\n",
    "    return Counter(preds).get(1, 0) / len(preds)\n",
    "\n",
    "\n",
    "def get_fit_params_at_n_samples(n, data_source):\n",
    "    concentration_list, caf_proportion_list = [], []\n",
    "    for conc, all_samples in data_source.items():\n",
    "        ratio = _predict_ratio_for_conc(n, all_samples)\n",
    "        if ratio is None:\n",
    "            continue\n",
    "        concentration_list.append(conc)\n",
    "        caf_proportion_list.append(ratio)\n",
    "    if len(concentration_list) < 2:\n",
    "        return None, None, None\n",
    "    logc = np.log10(concentration_list)\n",
    "    slope, intercept, r_value, *_ = linregress(logc, caf_proportion_list)\n",
    "    r2 = r_value**2\n",
    "    return slope, intercept, r2\n",
    "tfc_model.eval(); classifier.eval()\n",
    "slopes, intercepts, r2s = [], [], []\n",
    "\n",
    "for n in sample_points:\n",
    "    s, b, r2 = get_fit_params_at_n_samples(n, all_data_by_conc)\n",
    "    slopes.append(np.nan if s  is None else s)\n",
    "    intercepts.append(np.nan if b  is None else b)\n",
    "    r2s.append(np.nan if r2 is None else r2)\n",
    "    print(f\"  - n={n}: slope={s:.6f}, intercept={b:.6f}, R^2={r2:.4f}\")\n",
    "\n",
    "final_slope, final_intercept, final_r2 = get_fit_params_at_n_samples(min_samples_across_all_files, all_data_by_conc)\n",
    "print(f\"  - slope_all={final_slope:.6f}, intercept_all={final_intercept:.6f}, R^2_all={final_r2:.4f}\")\n",
    "\n",
    "eps = 1e-12\n",
    "fs = final_slope     if abs(final_slope)     > eps else eps\n",
    "fi = final_intercept if abs(final_intercept) > eps else eps\n",
    "fr = final_r2        if abs(final_r2)        > eps else eps  \n",
    "\n",
    "norm_slopes     = np.array(slopes, dtype=float)     / fs\n",
    "norm_intercepts = np.array(intercepts, dtype=float) / fi\n",
    "r2_array        = np.array(r2s, dtype=float)\n",
    "norm_r2         = r2_array / fr                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93250ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MultipleLocator, FixedLocator\n",
    "import matplotlib.gridspec as gridspec\n",
    "from math import floor\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.size': 24,\n",
    "    # 'font.family': 'Arial', \n",
    "    'font.weight': 'normal',\n",
    "    'xtick.labelsize': 24, \n",
    "    'ytick.labelsize': 24\n",
    "})\n",
    "\n",
    "def _interp_at_break(tp, yA, yB, break_point):\n",
    "    idxR = int(np.searchsorted(tp, break_point))\n",
    "    idxL = max(idxR - 1, 0)\n",
    "    valid_indices_A = np.where(~np.isnan(yA))[0]\n",
    "    valid_indices_B = np.where(~np.isnan(yB))[0]\n",
    "    \n",
    "    if idxR == 0 or idxR >= len(tp) or np.isclose(tp[idxL], tp[min(idxR, len(tp)-1)]):\n",
    "        yB_A = yA[idxL] if idxL in valid_indices_A else np.nan\n",
    "        yB_B = yB[idxL] if idxL in valid_indices_B else np.nan\n",
    "        tB = break_point\n",
    "    else:\n",
    "        t0, t1 = tp[idxL], tp[idxR]\n",
    "        w = (break_point - t0) / (t1 - t0)\n",
    "        yB_A = yA[idxL] + w * (yA[idxR] - yA[idxL]) if idxL in valid_indices_A and idxR in valid_indices_A else np.nan\n",
    "        yB_B = yB[idxL] + w * (yB[idxR] - yB[idxL]) if idxL in valid_indices_B and idxR in valid_indices_B else np.nan\n",
    "        tB = break_point\n",
    "\n",
    "    tL, A_L, B_L = np.r_[tp[:idxR], tB], np.r_[yA[:idxR], yB_A], np.r_[yB[:idxR], yB_B]\n",
    "    tR, A_R, B_R = np.r_[tB, tp[idxR:]], np.r_[yB_A, yA[idxR:]], np.r_[yB_B, yB[idxR:]]\n",
    "    return tB, (tL, A_L, B_L), (tR, A_R, B_R)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1])\n",
    "ax_left  = fig.add_subplot(gs[0])\n",
    "ax_right = fig.add_subplot(gs[1], sharey=ax_left) \n",
    "\n",
    "ax_left.spines['right'].set_visible(False)\n",
    "ax_right.spines['left'].set_visible(False)\n",
    "\n",
    "tp = np.asarray(time_points, dtype=float)\n",
    "ys = np.asarray(norm_slopes, dtype=float)\n",
    "yi = np.asarray(norm_intercepts, dtype=float)\n",
    "\n",
    "BAND = 0.05\n",
    "y_min = min(np.nanmin(ys), np.nanmin(yi), np.nanmin(norm_r2), 1 - BAND) - 0.025\n",
    "y_max = max(np.nanmax(ys), np.nanmax(yi), np.nanmax(norm_r2), 1 + BAND) + 0.025\n",
    "ax_left.set_ylim(y_min, y_max)\n",
    "ax_left.set_ylabel(\"Normalized Value\", fontsize=24, labelpad=6)\n",
    "\n",
    "for ax in (ax_left, ax_right):\n",
    "    ax.axhspan(1 - BAND, 1 + BAND, facecolor='gray', alpha=0.15, zorder=-20)\n",
    "    ax.axhline(1.0, color='black', linestyle='--', lw=1.3, zorder=-5)\n",
    "    ax.set_facecolor('none')\n",
    "\n",
    "BREAK_POINT = 20.0\n",
    "tB,  (tL,  yL,  iL), (tR,  yR,  iR)  = _interp_at_break(tp, ys,      yi,       BREAK_POINT)\n",
    "tB2, (tL2, r2L, _ ), (tR2, r2R, _ ) = _interp_at_break(tp, norm_r2,  norm_r2,  BREAK_POINT)\n",
    "\n",
    "SLOPE_COLOR     = 'crimson'     \n",
    "INTERCEPT_COLOR = 'royalblue'  \n",
    "R2_COLOR = 'forestgreen'\n",
    "ALPHA_VAL = 0.8\n",
    "line_s_left, = ax_left.plot(tL,  yL, 'o-',  lw=2.0, ms=7, color=SLOPE_COLOR,\n",
    "                            label='Slope',     zorder=5, alpha=ALPHA_VAL)\n",
    "line_i_left, = ax_left.plot(tL,  iL, 's--', lw=2.0, ms=6, color=INTERCEPT_COLOR,\n",
    "                            label='Intercept', zorder=5, alpha=ALPHA_VAL)\n",
    "line_r2_left,= ax_left.plot(tL2, r2L, '^-.', lw=2.0, ms=7, color=R2_COLOR,\n",
    "                            label=r'$R^2$',    zorder=4, alpha=ALPHA_VAL)\n",
    "\n",
    "ax_right.plot(tR,  yR,  'o-',  lw=2.0, ms=7, color=SLOPE_COLOR,     zorder=5, alpha=ALPHA_VAL)\n",
    "ax_right.plot(tR,  iR,  's--', lw=2.0, ms=6, color=INTERCEPT_COLOR, zorder=5, alpha=ALPHA_VAL)\n",
    "ax_right.plot(tR2, r2R, '^-.', lw=2.0, ms=7, color=R2_COLOR,        zorder=4, alpha=ALPHA_VAL)\n",
    "ax_left.plot(tB,  yL[-1],  'o', ms=8, color=SLOPE_COLOR,     zorder=6, alpha=ALPHA_VAL)\n",
    "ax_left.plot(tB,  iL[-1],  's', ms=7, color=INTERCEPT_COLOR, zorder=6, alpha=ALPHA_VAL)\n",
    "ax_left.plot(tB2, r2L[-1], '^', ms=8, color=R2_COLOR,        zorder=6, alpha=ALPHA_VAL)\n",
    "ax_right.plot(tB,  yR[0],  'o', ms=8, color=SLOPE_COLOR,     zorder=6, alpha=ALPHA_VAL)\n",
    "ax_right.plot(tB,  iR[0],  's', ms=7, color=INTERCEPT_COLOR, zorder=6, alpha=ALPHA_VAL)\n",
    "ax_right.plot(tB2, r2R[0], '^', ms=8, color=R2_COLOR,        zorder=6, alpha=ALPHA_VAL)\n",
    "\n",
    "ax_right.tick_params(labelleft=False, left=False)\n",
    "ax_left.set_xlim(0, BREAK_POINT)\n",
    "ax_left.xaxis.set_major_locator(FixedLocator([0, 5, 10, 15, 20]))\n",
    "ax_left.xaxis.set_minor_locator(MultipleLocator(1))\n",
    "x_max = float(np.max(time_points)) * 1.02\n",
    "ax_right.set_xlim(BREAK_POINT, x_max)\n",
    "ticks_right = [20, 40, 60, 90, 120]\n",
    "ax_right.xaxis.set_major_locator(FixedLocator(ticks_right))\n",
    "ax_right.xaxis.set_minor_locator(MultipleLocator(10))\n",
    "\n",
    "fig.subplots_adjust(wspace=0)\n",
    "ax_left.axvline(6, color='gray', linestyle='--', lw=1.6, zorder=-4)\n",
    "\n",
    "fig.supxlabel(\"Response time (s)\", fontsize=24, y=-0.012)\n",
    "ax_right.legend([line_s_left, line_i_left, line_r2_left],\n",
    "                ['Slope', 'Intercept', r'$R^2$'],\n",
    "                loc='lower right', fontsize=22, frameon=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7890e04a-b736-4e35-a946-e49175049a43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
